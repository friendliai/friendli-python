openapi: 3.1.0

servers:
  - url: https://inference.friendli.ai
    description: Friendli Serverless Endpoints.
  - url: https://inference.friendli.ai/dedicated
    description: Friendli Dedicated Endpoints.

tags:
  - name: Inference
  - name: Serverless

info:
  title: Friendli Endpoints API Reference
  version: v1
  termsOfService: https://friendli.ai/terms-of-service
  description: This is an OpenAPI reference of Friendli Endpoints API.
  contact:
    name: FriendliAI Support Team
    email: support@friendli.ai

paths:
  /v1/chat/completions:
    post:
      tags: [Inference]
      summary: Chat completion
      description: Given a list of messages forming a conversation, the model generates a response.
      operationId: ChatCompletion
      parameters:
        - $ref: '#/components/parameters/XFriendliTeam'
      requestBody:
        $ref: '#/components/requestBodies/ChatCompletion'
      responses:
        '200':
          $ref: '#/components/responses/ChatCompletionSuccess'

  /tools/v1/chat/completions:
    post:
      servers:
        - url: https://inference.friendli.ai
          description: Friendli Serverless Endpoints.
      tags: [Serverless]
      summary: Tool assisted chat completion
      description: Given a list of messages forming a conversation, the model generates a response. Additionally, the model can utilize built-in tools for tool calls, enhancing its capability to provide more comprehensive and actionable responses.
      operationId: ToolAssistedChatCompletion
      parameters:
        - $ref: '#/components/parameters/XFriendliTeam'
      requestBody:
        $ref: '#/components/requestBodies/ToolAssistedChatCompletion'
      responses:
        '200':
          $ref: '#/components/responses/ToolAssistedChatCompletionSuccess'

  /v1/completions:
    post:
      tags: [Inference]
      summary: Completion
      description: Generate text based on the given text prompt.
      operationId: Completion
      parameters:
        - $ref: '#/components/parameters/XFriendliTeam'
      requestBody:
        $ref: '#/components/requestBodies/Completion'
      responses:
        '200':
          $ref: '#/components/responses/CompletionSuccess'

  /v1/tokenize:
    post:
      tags: [Inference]
      summary: Tokenization
      description: By giving a text input, generate a tokenized output of token IDs.
      operationId: Tokenization
      parameters:
        - $ref: '#/components/parameters/XFriendliTeam'
      requestBody:
        $ref: '#/components/requestBodies/Tokenization'
      responses:
        '200':
          $ref: '#/components/responses/TokenizationSuccess'

  /v1/detokenize:
    post:
      tags: [Inference]
      summary: Detokenization
      description: By giving a list of tokens, generate a detokenized output text string.
      operationId: Detokenization
      parameters:
        - $ref: '#/components/parameters/XFriendliTeam'
      requestBody:
        $ref: '#/components/requestBodies/Detokenization'
      responses:
        '200':
          $ref: '#/components/responses/DetokenizationSuccess'

components:
  schemas:
    ChatCompletionRequestBody:
      type: object
      properties:
        model:
          type: string
          description: Code of the model to use. See [available model list](https://docs.friendli.ai/guides/serverless_endpoints/pricing#text-generation-models).
          examples: ['meta-llama-3.1-8b-instruct']
        messages:
          type: array
          items:
            $ref: '#/components/schemas/Message'
          description: A list of messages comprising the conversation so far.
          example:
            - role: system
              content: You are a helpful assistant.
            - role: user
              content: Hello!
        eos_token:
          type: ['array', 'null']
          items:
            type: integer
          description: A list of endpoint sentence tokens.
        frequency_penalty:
          type: ['number', 'null']
          description: Number between -2.0 and 2.0. Positive values penalizes tokens that have been sampled, taking into account their frequency in the preceding text. This penalization diminishes the model's tendency to reproduce identical lines verbatim.
        logit_bias:
          type: ['object', 'null']
          description: Accepts a JSON object that maps tokens to an associated bias value. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model.
        logprobs:
          type: ['boolean', 'null']
          description: Whether to return log probabilities of the output tokens or not.
        max_tokens:
          type: ['integer', 'null']
          description: The maximum number of tokens to generate. For decoder-only models like GPT, the length of your input tokens plus `max_tokens` should not exceed the model's maximum length (e.g., 2048 for OpenAI GPT-3). For encoder-decoder models like T5 or BlenderBot, `max_tokens` should not exceed the model's maximum output length. This is similar to Hugging Face's [`max_new_tokens`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.max_new_tokens) argument.
          examples: [200]
        min_tokens:
          type: ['integer', 'null']
          description: |
            The minimum number of tokens to generate. Default value is 0. This is similar to Hugging Face's [`min_new_tokens`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.generationconfig.min_new_tokens) argument.

            **This field is unsupported when `tools` are specified.**
          default: 0
        n:
          type: ['integer', 'null']
          description: The number of independently generated results for the prompt. Not supported when using beam search. Defaults to 1. This is similar to Hugging Face's [`num_return_sequences`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.num_return_sequences) argument.
          default: 1
        parallel_tool_calls:
          type: ['boolean', 'null']
          description: Whether to enable parallel function calling.
        presence_penalty:
          type: ['number', 'null']
          description: Number between -2.0 and 2.0. Positive values penalizes tokens that have been sampled at least once in the existing text.
        repetition_penalty:
          type: ['number', 'null']
          description: Penalizes tokens that have already appeared in the generated result (plus the input tokens for decoder-only models). Should be greater than or equal to 1.0 (1.0 means no penalty). See [keskar et al., 2019](https://arxiv.org/abs/1909.05858) for more details. This is similar to Hugging Face's [`repetition_penalty`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.generationconfig.repetition_penalty) argument.
        response_format:
          $ref: '#/components/schemas/TextResponseFormat'
        seed:
          type: ['array', 'null']
          items:
            type: integer
          description: Seed to control random procedure. If nothing is given, random seed is used for sampling, and return the seed along with the generated result. When using the `n` argument, you can pass a list of seed values to control all of the independent generations.
        stop:
          type: ['array', 'null']
          items:
            type: string
          description: When one of the stop phrases appears in the generation result, the API will stop generation. The stop phrases are excluded from the result. Defaults to empty list.
        stream:
          type: ['boolean', 'null']
          description: Whether to stream generation result. When set true, each token will be sent as [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format) once generated.
        stream_options:
          type: ['object', 'null']
          properties:
            include_usage:
              type: ['boolean', 'null']
              description: |
                When set to `true`,
                the number of tokens used will be included at the end of the stream result in the form of
                `"usage": {"completion_tokens": number, "prompt_tokens": number, "total_tokens": number}`.
          description: |
            Options related to stream.
            It can only be used when `stream: true`.
        temperature:
          type: ['number', 'null']
          description: Sampling temperature. Smaller temperature makes the generation result closer to greedy, argmax (i.e., `top_k = 1`) sampling. Defaults to 1.0. This is similar to Hugging Face's [`temperature`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.generationconfig.temperature) argument.
          default: 1.0
        timeout_microseconds:
          type: ['integer', 'null']
          description: Request timeout. Gives the `HTTP 429 Too Many Requests` response status code. Default behavior is no timeout.
        tool_choice:
          oneOf:
            - title: string
              type: ['string', 'null']
            - title: object
              type: ['object', 'null']
              properties:
                type:
                  type: string
                  enum:
                    - function
                  description: The type of the tool. Currently, only `function` is supported.
                function:
                  type: object
                  properties:
                    name:
                      type: string
                      description: The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
                  required:
                    - name
              required:
                - type
                - function
          description: |
            Determines the tool calling behavior of the model.
            When set to `none`, the model will bypass tool execution and generate a response directly.
            In `auto` mode (the default), the model dynamically decides whether to call a tool or respond with a message.
            Alternatively, setting `required` ensures that the model invokes at least one tool before responding to the user.
            You can also specify a particular tool by `{"type": "function", "function": {"name": "my_function"}}`.
        tools:
          type: ['array', 'null']
          items:
            $ref: '#/components/schemas/Tool'
          description: |
            A list of tools the model may call.
            Currently, only functions are supported as a tool.
            A maximum of 128 functions is supported.
            Use this to provide a list of functions the model may generate JSON inputs for.

            **When `tools` are specified, `min_tokens` field is unsupported.**
        top_k:
          type: ['integer', 'null']
          description: The number of highest probability tokens to keep for sampling. Numbers between 0 and the vocab size of the model (both inclusive) are allowed. The default value is 0, which means that the API does not apply top-k filtering. This is similar to Hugging Face's [`top_k`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.top_k) argument.
          default: 0
        top_logprobs:
          type: ['integer', 'null']
          description: The number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to true if this parameter is used.
        top_p:
          type: ['number', 'null']
          description: Tokens comprising the top `top_p` probability mass are kept for sampling. Numbers between 0.0 (exclusive) and 1.0 (inclusive) are allowed. Defaults to 1.0. This is similar to Hugging Face's [`top_p`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.top_p) argument.
          default: 1.0
      required:
        - model
        - messages
    ToolAssistedCompletionRequestBody:
      type: object
      properties:
        model:
          type: string
          description: Code of the model to use. See [available model list](https://docs.friendli.ai/guides/serverless_endpoints/pricing#text-generation-models).
          examples: ['meta-llama-3.1-8b-instruct']
        messages:
          type: array
          items:
            $ref: '#/components/schemas/Message'
          description: A list of messages comprising the conversation so far.
          example:
            - role: system
              content: You are a helpful assistant.
            - role: user
              content: Hello!
        eos_token:
          type: ['array', 'null']
          items:
            type: integer
          description: A list of endpoint sentence tokens.
        frequency_penalty:
          type: ['number', 'null']
          description: Number between -2.0 and 2.0. Positive values penalizes tokens that have been sampled, taking into account their frequency in the preceding text. This penalization diminishes the model's tendency to reproduce identical lines verbatim.
        max_tokens:
          type: ['integer', 'null']
          description: The maximum number of tokens to generate. For decoder-only models like GPT, the length of your input tokens plus `max_tokens` should not exceed the model's maximum length (e.g., 2048 for OpenAI GPT-3). For encoder-decoder models like T5 or BlenderBot, `max_tokens` should not exceed the model's maximum output length. This is similar to Hugging Face's [`max_new_tokens`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.max_new_tokens) argument.
          examples: [200]
        min_tokens:
          type: ['integer', 'null']
          description: |
            The minimum number of tokens to generate. Default value is 0. This is similar to Hugging Face's [`min_new_tokens`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.generationconfig.min_new_tokens) argument.

            **This field is unsupported when `tools` are specified.**
          default: 0
        n:
          type: ['integer', 'null']
          description: The number of independently generated results for the prompt. Not supported when using beam search. Defaults to 1. This is similar to Hugging Face's [`num_return_sequences`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.num_return_sequences) argument.
          default: 1
        parallel_tool_calls:
          type: ['boolean', 'null']
          description: Whether to enable parallel function calling.
        presence_penalty:
          type: ['number', 'null']
          description: Number between -2.0 and 2.0. Positive values penalizes tokens that have been sampled at least once in the existing text.
        repetition_penalty:
          type: ['number', 'null']
          description: Penalizes tokens that have already appeared in the generated result (plus the input tokens for decoder-only models). Should be greater than or equal to 1.0 (1.0 means no penalty). See [keskar et al., 2019](https://arxiv.org/abs/1909.05858) for more details. This is similar to Hugging Face's [`repetition_penalty`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.generationconfig.repetition_penalty) argument.
        response_format:
          $ref: '#/components/schemas/TextResponseFormat'
        resume_generation:
          type: ['boolean', 'null']
          description: |
            Enable to continue text generation even after an error occurs during a tool call.

            Note that enabling this option may use more tokens, as the system generates additional content to handle errors gracefully.
            However, if the system fails more than 8 times, the generation will stop regardless.

            ***Tip***
            This is useful in scenarios where you want to maintain text generation flow despite errors, such as when generating long-form content.
            The user will not be interrupted by tool call issues, ensuring a smoother experience.
        seed:
          type: ['array', 'null']
          items:
            type: integer
          description: Seed to control random procedure. If nothing is given, random seed is used for sampling, and return the seed along with the generated result. When using the `n` argument, you can pass a list of seed values to control all of the independent generations.
        stop:
          type: ['array', 'null']
          items:
            type: string
          description: When one of the stop phrases appears in the generation result, the API will stop generation. The stop phrases are excluded from the result. Defaults to empty list.
        stream:
          type: ['boolean', 'null']
          description: |
            Whether to stream generation result. When set true, each token will be sent as [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format) once generated.

            **Caution: `stream: false` is unsupported now.**
        temperature:
          type: ['number', 'null']
          description: Sampling temperature. Smaller temperature makes the generation result closer to greedy, argmax (i.e., `top_k = 1`) sampling. Defaults to 1.0. This is similar to Hugging Face's [`temperature`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.generationconfig.temperature) argument.
          default: 1.0
        timeout_microseconds:
          type: ['integer', 'null']
          description: Request timeout. Gives the `HTTP 429 Too Many Requests` response status code. Default behavior is no timeout.
        tool_choice:
          oneOf:
            - title: string
              type: ['string', 'null']
            - title: object
              type: ['object', 'null']
              properties:
                type:
                  type: string
                  enum:
                    - function
                  description: The type of the tool. Currently, only `function` is supported.
                function:
                  type: object
                  properties:
                    name:
                      type: string
                      description: The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
                  required:
                    - name
              required:
                - type
                - function
          description: |
            Determines the tool calling behavior of the model.
            When set to `none`, the model will bypass tool execution and generate a response directly.
            In `auto` mode (the default), the model dynamically decides whether to call a tool or respond with a message.
            Alternatively, setting `required` ensures that the model invokes at least one tool before responding to the user.
            You can also specify a particular tool by `{"type": "function", "function": {"name": "my_function"}}`.
        tools:
          type: ['array', 'null']
          items:
            $ref: '#/components/schemas/ToolForToolAssistedChat'
          description: |
            A list of tools the model may call.
            A maximum of 128 functions is supported.
            Use this to provide a list of functions the model may generate JSON inputs for.
            For more detailed information about each tool, please refer [here](https://docs.friendli.ai/guides/serverless_endpoints/tools/built_in_tools).

            **When `tools` are specified, `min_tokens` field is unsupported.**
        top_k:
          type: ['integer', 'null']
          description: The number of highest probability tokens to keep for sampling. Numbers between 0 and the vocab size of the model (both inclusive) are allowed. The default value is 0, which means that the API does not apply top-k filtering. This is similar to Hugging Face's [`top_k`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.top_k) argument.
          default: 0
        top_p:
          type: ['number', 'null']
          description: Tokens comprising the top `top_p` probability mass are kept for sampling. Numbers between 0.0 (exclusive) and 1.0 (inclusive) are allowed. Defaults to 1.0. This is similar to Hugging Face's [`top_p`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.top_p) argument.
          default: 1.0
      required:
        - model
        - messages
    CompletionRequestBody:
      oneOf:
        - title: prompt
          $ref: '#/components/schemas/CompletionRequestBodyWithPrompt'
        - title: tokens
          $ref: '#/components/schemas/CompletionRequestBodyWithTokens'
    TokenizationRequestBody:
      type: object
      properties:
        model:
          type: string
          description: Code of the model to use. See [available model list](https://docs.friendli.ai/guides/serverless_endpoints/pricing#text-generation-models).
          example: 'meta-llama-3.1-8b-instruct'
        prompt:
          type: string
          description: Input text prompt to tokenize.
          example: 'What is generative AI?'
      required:
        - model
        - prompt
    DetokenizationRequestBody:
      type: object
      properties:
        model:
          type: string
          description: Code of the model to use. See [available model list](https://docs.friendli.ai/guides/serverless_endpoints/pricing#text-generation-models).
          example: 'meta-llama-3.1-8b-instruct'
        tokens:
          type: array
          items:
            type: integer
          description: A token sequence to detokenize.
          example: [128000, 3923, 374, 1803, 1413, 15592, 30]

    ChatCompletionResponse:
      type: object
      required: [choices, usage]
      properties:
        choices:
          type: array
          items:
            $ref: '#/components/schemas/ChatCompletionChoice'
        usage:
          $ref: '#/components/schemas/Usage'
        created:
          type: integer
          description: The Unix timestamp (in seconds) for when the generation completed.
    StreamedChatCompletionResponse:
      description: A server-sent event containing chat completion content.
      type: object
      required: [data]
      properties:
        data:
          type: object
          required: [choices, created]
          properties:
            choices:
              type: array
              items:
                $ref: '#/components/schemas/StreamedChatCompletionChoice'
            usage:
              $ref: '#/components/schemas/Usage'
            created:
              type: integer
              description: The Unix timestamp (in seconds) for when the token sampled.
    StreamedToolAssistedChatCompletionResponse:
      description: A server-sent event containing chat completion content.
      type: object
      required: [data]
      properties:
        data:
          type: object
          required: [choices, created]
          properties:
            choices:
              type: array
              items:
                $ref: '#/components/schemas/StreamedChatCompletionChoice'
            created:
              type: integer
              description: The Unix timestamp (in seconds) for when the token sampled.
    CompletionResponse:
      type: object
      required: [choices, usage]
      properties:
        choices:
          type: array
          items:
            $ref: '#/components/schemas/CompletionChoice'
        usage:
          $ref: '#/components/schemas/Usage'
    StreamedCompletionResponse:
      type: object
      required: [data]
      properties:
        data:
          oneOf:
            - title: token_sampled
              $ref: '#/components/schemas/StreamedCompletionTokenSampled'
            - title: complete
              $ref: '#/components/schemas/StreamedCompletionTokenComplete'
          discriminator:
            propertyName: event
            mapping:
              token_sampled: '#/components/schemas/StreamedCompletionTokenSampled'
              complete: '#/components/schemas/StreamedCompletionTokenComplete'
    TokenizationResponse:
      type: object
      properties:
        tokens:
          type: array
          items:
            type: integer
            description: A token ID.
          description: A list of token IDs.
    DetokenizationResponse:
      type: object
      properties:
        text:
          type: string
          description: Detokenized text output.

    CompletionRequestBodyWithPrompt:
      allOf:
        - type: object
          properties:
            prompt:
              type: string
              description: The prompt (i.e., input text) to generate completion for. Either `prompt` or `tokens` field is required.
              examples: ['Say this is a test!']
          required:
            - prompt
        - $ref: '#/components/schemas/CommonCompletionRequestBody'
    CompletionRequestBodyWithTokens:
      allOf:
        - type: object
          properties:
            tokens:
              type: array
              items:
                type: integer
              description: The tokenized prompt (i.e., input tokens). Either `prompt` or `tokens` field is required.
          required:
            - tokens
        - $ref: '#/components/schemas/CommonCompletionRequestBody'
    CommonCompletionRequestBody:
      type: object
      properties:
        model:
          type: string
          description: Code of the model to use. See [available model list](https://docs.friendli.ai/guides/serverless_endpoints/pricing#text-generation-models).
          examples: ['meta-llama-3.1-8b-instruct']
        bad_word_tokens:
          type: ['array', 'null']
          items:
            $ref: '#/components/schemas/TokenSequence'
          description: Same as the above `bad_words` field, but receives token sequences instead of text phrases. This is similar to Hugging Face's [`bad_word_ids`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.bad_words_ids) argument.
        bad_words:
          type: ['array', 'null']
          items:
            type: string
          description: |
            Text phrases that should not be generated.
            For a bad word phrase that contains N tokens, if the first N-1 tokens appears at the last of the generated result, the logit for the last token of the phrase is set to -inf.
            Before checking whether a bard word is included in the result, the word is converted into tokens.
            We recommend using `bad_word_tokens` because it is clearer.
            For example, after tokenization, phrases "clear" and " clear" can result in different token sequences due to the prepended space character.
            Defaults to empty list.
        beam_compat_no_post_normalization:
          type: ['boolean', 'null']
        beam_compat_pre_normalization:
          type: ['boolean', 'null']
        beam_search_type:
          type: ['string', 'null']
          description: One of `DETERMINISTIC`, `NAIVE_SAMPLING`, and `STOCHASTIC`. Which beam search type to use. `DETERMINISTIC` means the standard, deterministic beam search, which is similar to Hugging Face's [`beam_search`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationMixin.beam_search). Argmuents for controlling random sampling such as `top_k` and `top_p` are not allowed for this option. `NAIVE_SAMPLING` is similar to Hugging Face's [`beam_sample`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationMixin.beam_sample). `STOCHASTIC` means stochastic beam search (more details in [Kool et al. (2019)](https://proceedings.mlr.press/v97/kool19a.html)). This option is ignored if `num_beams` is not provided. Defaults to `DETERMINISTIC`.
          default: 'DETERMINISTIC'
        early_stopping:
          type: ['boolean', 'null']
          description: Whether to stop the beam search when at least `num_beams` beams are finished with the EOS token. Only allowed for beam search. Defaults to false. This is similar to Hugging Face's [`early_stopping`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.early_stopping) argument.
          default: false
        embedding_to_replace:
          type: ['array', 'null']
          items:
            type: number
          description: A list of flattened embedding vectors used for replacing the tokens at the specified indices provided via `token_index_to_replace`.
        encoder_no_repeat_ngram:
          type: ['integer', 'null']
          description: If this exceeds 1, every ngram of that size occurring in the input token sequence cannot appear in the generated result. 1 means that this mechanism is disabled (i.e., you cannot prevent 1-gram from being generated repeatedly). Only allowed for encoder-decoder models. Defaults to 1. This is similar to Hugging Face's [`encoder_no_repeat_ngram_size`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.encoder_no_repeat_ngram_size) argument.
          default: 1
        encoder_repetition_penalty:
          type: ['number', 'null']
          description: Penalizes tokens that have already appeared in the input tokens. Should be greater than or equal to 1.0. 1.0 means no penalty. Only allowed for encoder-decoder models. See [Keskar et al., 2019](https://arxiv.org/abs/1909.05858) for more details. This is similar to Hugging Face's [`encoder_repetition_penalty`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.encoder_repetition_penalty) argument.
        eos_token:
          type: ['array', 'null']
          items:
            type: integer
          description: A list of endpoint sentence tokens.
        forced_output_tokens:
          type: ['array', 'null']
          items:
            type: integer
          description: A token sequence that is enforced as a generation output. This option can be used when evaluating the model for the datasets with multi-choice problems (e.g., [HellaSwag](https://huggingface.co/datasets/hellaswag), [MMLU](https://huggingface.co/datasets/cais/mmlu)). Use this option with `include_output_logprobs` to get logprobs for the evaluation.
        frequency_penalty:
          type: ['number', 'null']
          description: Number between -2.0 and 2.0. Positive values penalizes tokens that have been sampled, taking into account their frequency in the preceding text. This penalization diminishes the model's tendency to reproduce identical lines verbatim.
        include_output_logits:
          type: ['boolean', 'null']
          description: Whether to include the output logits to the generation output.
        include_output_logprobs:
          type: ['boolean', 'null']
          description: Whether to include the output logprobs to the generation output.
        length_penalty:
          type: ['number', 'null']
          description: Coefficient for exponential length penalty that is used with beam search. Only allowed for beam search. Defaults to 1.0. This is similar to Hugging Face's [`length_penalty`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.length_penalty) argument.
        max_tokens:
          type: ['integer', 'null']
          description: The maximum number of tokens to generate. For decoder-only models like GPT, the length of your input tokens plus `max_tokens` should not exceed the model's maximum length (e.g., 2048 for OpenAI GPT-3). For encoder-decoder models like T5 or BlenderBot, `max_tokens` should not exceed the model's maximum output length. This is similar to Hugging Face's [`max_new_tokens`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.max_new_tokens) argument.
          examples: [200]
        max_total_tokens:
          type: ['integer', 'null']
          description: The maximum number of tokens including both the generated result and the input tokens. Only allowed for decoder-only models. Only one argument between `max_tokens` and `max_total_tokens` is allowed. Default value is the model's maximum length. This is similar to Hugging Face's [`max_length`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.max_length) argument.
        min_tokens:
          type: ['integer', 'null']
          description: The minimum number of tokens to generate. Default value is 0. This is similar to Hugging Face's [`min_new_tokens`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.generationconfig.min_new_tokens) argument.
          default: 0
        min_total_tokens:
          type: ['integer', 'null']
          description: The minimum number of tokens including both the generated result and the input tokens. Only allowed for decoder-only models. Only one argument between `min_tokens` and `min_total_tokens` is allowed. This is similar to Hugging Face's [`min_length`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.min_length) argument.
        n:
          type: ['integer', 'null']
          description: The number of independently generated results for the prompt. Not supported when using beam search. Defaults to 1. This is similar to Hugging Face's [`num_return_sequences`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.num_return_sequences) argument.
          default: 1
        no_repeat_ngram:
          type: ['integer', 'null']
          description: If this exceeds 1, every ngram of that size can only occur once among the generated result (plus the input tokens for decoder-only models). 1 means that this mechanism is disabled (i.e., you cannot prevent 1-gram from being generated repeatedly). Defaults to 1. This is similar to Hugging Face's [`no_repeat_ngram_size`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.no_repeat_ngram_size) argument.
          default: 1
        num_beams:
          type: ['integer', 'null']
          description: Number of beams for beam search. Numbers between 1 and 31 (both inclusive) are allowed. Default behavior is no beam search. This is similar to Hugging Face's [`num_beams`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.num_beams) argument.
        presence_penalty:
          type: ['number', 'null']
          description: Number between -2.0 and 2.0. Positive values penalizes tokens that have been sampled at least once in the existing text.
        repetition_penalty:
          type: ['number', 'null']
          description: Penalizes tokens that have already appeared in the generated result (plus the input tokens for decoder-only models). Should be greater than or equal to 1.0 (1.0 means no penalty). See [keskar et al., 2019](https://arxiv.org/abs/1909.05858) for more details. This is similar to Hugging Face's [`repetition_penalty`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.generationconfig.repetition_penalty) argument.
        response_format:
          $ref: '#/components/schemas/TextResponseFormat'
        seed:
          type: ['array', 'null']
          items:
            type: integer
          description: Seed to control random procedure. If nothing is given, the API generate the seed randomly, use it for sampling, and return the seed along with the generated result. When using the `n` argument, you can pass a list of seed values to control all of the independent generations.
        stop:
          type: ['array', 'null']
          items:
            type: string
          description: |
            When one of the stop phrases appears in the generation result, the API will stop generation.
            The stop phrases are excluded from the result.
            This option is incompatible with beam search (specified by `num_beams`); use `stop_tokens` for that case instead.
            Defaults to empty list.
        stop_tokens:
          type: ['array', 'null']
          items:
            $ref: '#/components/schemas/TokenSequence'
          description: |
            Stop generating further tokens when generated token corresponds to any of the tokens in the sequence.
            If beam search is enabled, all of the active beams should contain the stop token to terminate generation.
        stream:
          type: ['boolean', 'null']
          description: Whether to stream generation result. When set true, each token will be sent as [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format) once generated. Not supported when using beam search.
        temperature:
          type: ['number', 'null']
          description: Sampling temperature. Smaller temperature makes the generation result closer to greedy, argmax (i.e., `top_k = 1`) sampling. Defaults to 1.0. This is similar to Hugging Face's [`temperature`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.generationconfig.temperature) argument.
          default: 1.0
        timeout_microseconds:
          type: ['integer', 'null']
          description: Request timeout. Gives the `HTTP 429 Too Many Requests` response status code. Default behavior is no timeout.
        token_index_to_replace:
          type: ['array', 'null']
          items:
            type: integer
          description: A list of token indices where to replace the embeddings of input tokens provided via either `tokens` or `prompt`.
        top_k:
          type: ['integer', 'null']
          description: The number of highest probability tokens to keep for sampling. Numbers between 0 and the vocab size of the model (both inclusive) are allowed. The default value is 0, which means that the API does not apply top-k filtering. This is similar to Hugging Face's [`top_k`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.top_k) argument.
          examples: [1]
          default: 0
        top_p:
          type: ['number', 'null']
          description: Tokens comprising the top `top_p` probability mass are kept for sampling. Numbers between 0.0 (exclusive) and 1.0 (inclusive) are allowed. Defaults to 1.0. This is similar to Hugging Face's [`top_p`](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.top_p) argument.
          default: 1.0
      required:
        - model

    ChatCompletionChoice:
      type: object
      required: [index, message, finish_reason]
      properties:
        index:
          type: integer
          description: The index of the choice in the list of generated choices.
          examples: [0]
        message:
          type: object
          required: [role]
          properties:
            role:
              type: string
              description: Role of the generated message author, in this case `assistant`.
            content:
              type: string
              description: The contents of the assistant message.
            tool_calls:
              type: array
              items:
                type: object
                required: [id, type, function]
                properties:
                  id:
                    type: string
                    description: The ID of the tool call.
                  type:
                    type: string
                    enum:
                      - function
                    description: The type of the tool.
                  function:
                    type: object
                    required: [name, arguments]
                    properties:
                      name:
                        type: string
                        description: The name of the function to call.
                      arguments:
                        type: string
                        description: |
                          The arguments for calling the function, generated by the model in JSON format.
                          Ensure to validate these arguments in your code before invoking the function since the model may not always produce valid JSON.
        finish_reason:
          type: string
          description: |
            Termination condition of the generation. `stop` means the API returned the full chat completion generated by the model without running into any limits.
            `length` means the generation exceeded `max_tokens` or the conversation exceeded the max context length.
            `tool_calls` means the API has generated tool calls.
        logprobs:
          $ref: '#/components/schemas/Logprobs'
    StreamedChatCompletionChoice:
      type: object
      required: [index, delta]
      properties:
        index:
          type: integer
          description: The index of the choice in the list of generated choices.
          examples: [0]
        delta:
          type: object
          properties:
            role:
              type: string
              description: Role of the generated message author, in this case `assistant`.
            content:
              type: string
              description: The contents of the assistant message.
            tool_calls:
              type: object
              required: [index, id, type, function]
              properties:
                index:
                  type: integer
                  description: The index of tool call being generated.
                id:
                  type: string
                  description: The ID of the tool call.
                type:
                  type: string
                  enum:
                    - function
                  description: The type of the tool.
                function:
                  type: object
                  required: [name, arguments]
                  properties:
                    name:
                      type: string
                      description: The name of the function to call.
                    arguments:
                      type: string
                      description: |
                        The arguments for calling the function, generated by the model in JSON format.
                        Ensure to validate these arguments in your code before invoking the function since the model may not always produce valid JSON.
        finish_reason:
          type: ['string', 'null']
          description: Termination condition of the generation. `stop` means the API returned the full chat completion generated by the model without running into any limits. `length` means the generation exceeded `max_tokens` or the conversation exceeded the max context length.
        logprobs:
          $ref: '#/components/schemas/Logprobs'
    ToolAssistedChatToolStatusEvent:
      type: object
      required: [tool_call_id, name, status, parameters, result, timestamp]
      properties:
        tool_call_id:
          type: string
          description: The ID of the tool call.
        name:
          type: string
          enum:
            - math:calculator
            - math:statistics
            - math:calendar
            - web:search
            - web:url
            - code:python-interpreter
            - file:text
          description: The name of the built-in tool.
        status:
          type: string
          enum:
            - STARTED
            - UPDATING
            - ENDED
            - ERRORED
          description: Indicates the current execution status of the tool.
        parameters:
          type: array
          items:
            type: object
            required: [name, value]
            properties:
              name:
                type: string
                description: The name of the tool’s function parameter.
              value:
                type: string
                description: The value of the tool’s function parameter.
        result:
          type: string
          description: The output from the tool’s execution.
        files:
          type: array
          items:
            type: object
            required: [name, url]
            properties:
              name:
                type: string
                description: The name of the file generated by the tool’s execution.
              url:
                type: string
                description: URL of the file generated by the tool’s execution.
        message:
          type: string
          description: Message generated by the tool’s execution.
        error:
          type: object
          required: [type, msg]
          properties:
            type:
              type: string
              description: The type of error encountered during the tool’s execution.
            msg:
              type: string
              description: The message of error.
        timestamp:
          type: number
          description: The Unix timestamp (in seconds) for when the event occurred.
    CompletionChoice:
      type: object
      required: [index, seed, text, tokens]
      properties:
        index:
          type: integer
          description: The index of the choice in the list of generated choices.
          examples: [0]
        seed:
          type: integer
          description: Random seed used for the generation.
          examples: [42]
        text:
          type: string
          description: Generated text output.
          examples: ['This is indeed a test']
        tokens:
          type: array
          items:
            type: integer
          description: Generated output tokens.
          example: [128000, 2028, 374, 13118, 264, 1296]
    StreamedCompletionTokenSampled:
      type: object
      required: [event, index, text, token]
      properties:
        event:
          type: string
          enum: [token_sampled]
          description: Type of server-sent event.
        index:
          type: integer
          description: The index of the choice in the list of generated choices.
        text:
          type: string
          description: Generated text output.
        token:
          type: integer
          description: Generated output token.
    StreamedCompletionTokenComplete:
      type: object
      required: [event, choices, usage]
      properties:
        event:
          type: string
          enum: [complete]
          description: Type of server-sent event.
        choices:
          type: array
          items:
            $ref: '#/components/schemas/CompletionChoice'
        usage:
          $ref: '#/components/schemas/Usage'

    TokenSequence:
      type: object
      properties:
        tokens:
          type: array
          items:
            type: integer
          description: A List of token IDs.
    TextResponseFormat:
      type: ['object', 'null']
      description: |
        The enforced format of the model's output.

        Note that the content of the output message may be truncated if it exceeds the `max_tokens`.
        You can check this by verifying that the `finish_reason` of the output message is `length`.

        ***Important***
        You must explicitly instruct the model to produce the desired output format using a system prompt or user message (e.g., `You are an API generating a valid JSON as output.`).
        Otherwise, the model may result in an unending stream of whitespace or other characters.
      properties:
        type:
          type: string
          enum:
            - text
            - json_object
            - regex
          description: Type of the response format.
        schema:
          type: string
          description: |
            The schema of the output. For `{ "type": "json_object" }`, `schema` should be a serialized string of JSON schema. For `{ "type": "regex" }`, `schema` should be a regex pattern.

            ***Caveat***
            For the JSON object type, recursive definitions are not supported. Optional properties are also not supported; all properties of `{ "type": "object" }` are generated regardless of whether they are listed in the `required` field.
            For the regex type, lookaheads/lookbehinds (e.g., `\a`, `\z`, `^`, `$`, `(?=)`, `(?!)`, `(?<=...)`, `(?<!...)`) are not supported. Group specials (e.g., `\w`, `\W`, `\d`, `\D`, `\s`, `\S`) do not support non-ASCII characters. Unicode escape patterns (e.g., `\N`, `\p`, `\P`) are not supported. Additionally, conditional matching (`(?(`) and back-references can cause inefficiency.
      required:
        - type
    Message:
      oneOf:
        - title: system
          $ref: '#/components/schemas/SystemMessage'
        - title: user
          $ref: '#/components/schemas/UserMessage'
        - title: assistant
          $ref: '#/components/schemas/AssistantMessage'
        - title: tool
          $ref: '#/components/schemas/ToolMessage'
      discriminator:
        propertyName: role
        mapping:
          system: '#/components/schemas/SystemMessage'
          user: '#/components/schemas/UserMessage'
          assistant: '#/components/schemas/AssistantMessage'
          tool: '#/components/schemas/ToolMessage'
    SystemMessage:
      type: object
      properties:
        role:
          type: string
          enum:
            - system
          description: The role of the messages author.
        content:
          type: string
          description: The content of system message.
        name:
          type: string
          description: The name for the participant to distinguish between participants with the same role.
      required:
        - role
        - content
    UserMessage:
      type: object
      properties:
        role:
          type: string
          enum:
            - user
          description: The role of the messages author.
        content:
          type: string
          description: The content of user message.
        name:
          type: string
          description: The name for the participant to distinguish between participants with the same role.
      required:
        - role
        - content
    AssistantMessage:
      type: object
      properties:
        role:
          type: string
          enum:
            - assistant
          description: The role of the messages author.
        content:
          type: string
          description: The content of assistant message. Required unless `tool_calls` is specified.
        name:
          type: string
          description: The name for the participant to distinguish between participants with the same role.
        tool_calls:
          type: array
          items:
            type: object
            properties:
              id:
                type: string
                description: The ID of tool call.
              type:
                type: string
                description: The type of tool call.
                enum:
                  - function
              function:
                type: object
                properties:
                  name:
                    type: string
                    description: The name of function
                  arguments:
                    type: string
                    description: The arguments of function in JSON schema format to call the function.
                required:
                  - name
                  - arguments
                description: The function specification
            required:
              - id
              - type
              - function
      required:
        - role
    ToolMessage:
      type: object
      properties:
        role:
          type: string
          enum:
            - tool
          description: The role of the messages author.
        content:
          type: string
          description: The content of tool message that contains the result of tool calling.
        tool_call_id:
          type: string
          description: The ID of tool call corresponding to this message.
        name:
          type: string
          description: An optional name of the tool call corresponding to this message.
      required:
        - role
        - content
        - tool_call_id
    Tool:
      type: object
      properties:
        type:
          type: string
          enum:
            - function
          description: The type of the tool. Currently, only `function` is supported.
        function:
          $ref: '#/components/schemas/Function'
      required:
        - type
        - function
    ToolForToolAssistedChat:
      oneOf:
        - title: function
          $ref: '#/components/schemas/FunctionTool'
        - title: file
          $ref: '#/components/schemas/FileBuiltInTool'
        - title: others
          $ref: '#/components/schemas/OtherBuiltInTool'
      discriminator:
        propertyName: type
        mapping:
          function: '#/components/schemas/FunctionTool'
          file:text: '#/components/schemas/FileBuiltInTool'
          math:calculator: '#/components/schemas/OtherBuiltInTool'
    FunctionTool:
      type: object
      properties:
        type:
          type: string
          enum:
            - function
          description: The type of the tool.
        function:
          $ref: '#/components/schemas/Function'
      required:
        - type
        - function
    FileBuiltInTool:
      type: object
      properties:
        type:
          type: string
          enum:
            - file:text
          description: The type of the file parser tool. Only .txt and .pdf files are supported.
        files:
          type: array
          items:
            type: string
          description: A List of file IDs.
      required:
        - type
        - files
    OtherBuiltInTool:
      type: object
      properties:
        type:
          type: string
          enum:
            - math:calculator
            - math:statistics
            - math:calendar
            - web:search
            - web:url
            - code:python-interpreter
          description: The type of the built-in tool.
      required:
        - type
    Function:
      type: object
      properties:
        description:
          type: ['string', 'null']
          description: A description of what the function does, used by the model to choose when and how to call the function.
        name:
          type: string
          description: The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
        parameters:
          type: object
          description: |
            The parameters the functions accepts, described as a JSON Schema object.
            To represent a function with no parameters, use the value `{"type": "object", "properties": {}}`.
      required:
        - name
        - parameters
    Usage:
      type: object
      required: [prompt_tokens, completion_tokens, total_tokens]
      properties:
        prompt_tokens:
          type: integer
          description: Number of tokens in the prompt.
          examples: [5]
        completion_tokens:
          type: integer
          description: Number of tokens in the generated completion.
          examples: [7]
        total_tokens:
          type: integer
          description: Total number of tokens used in the request (`prompt_tokens` + `completion_tokens`).
          examples: [12]
    Logprobs:
      type: object
      description: Log probability information for the choice.
      properties:
        content:
          type: array
          description: A list of message content tokens with log probability information.
          items:
            type: object
            properties:
              token:
                type: string
                description: The token.
              logprob:
                type: number
                description: The log probability of this token.
              top_logprobs:
                type: array
                description: List of the most likely tokens and their log probability, at this token position.
                items:
                  type: object
                  properties:
                    token:
                      type: string
                      description: The token.
                    logprob:
                      type: number
                      description: The log probability of this token.

  parameters:
    XFriendliTeam:
      name: X-Friendli-Team
      in: header
      description: ID of team to run requests as (optional parameter).
      required: false
      schema:
        type: string

  requestBodies:
    ChatCompletion:
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/ChatCompletionRequestBody'
          example:
            model: 'meta-llama-3.1-8b-instruct'
            message:
              - role: system
                content: You are a helpful assistant.
              - role: user
                content: Hello!

    ToolAssistedChatCompletion:
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/ToolAssistedCompletionRequestBody'
          example:
            model: 'meta-llama-3.1-8b-instruct'
            message:
              - role: system
                content: You are a helpful assistant.
              - role: user
                content: Hello!
            tools:
              - type: 'math:calculator'
              - type: 'web:search'

    Completion:
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/CompletionRequestBody'
          example:
            model: 'meta-llama-3.1-8b-instruct'
            prompt: 'Say this is a test!'

    Tokenization:
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/TokenizationRequestBody'
          example:
            model: 'meta-llama-3.1-8b-instruct'
            prompt: 'What is generative AI?'

    Detokenization:
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/DetokenizationRequestBody'
          example:
            model: 'meta-llama-3.1-8b-instruct'
            tokens: [128000, 3923, 374, 1803, 1413, 15592, 30]

  responses:
    ChatCompletionSuccess:
      description: Successfully generated a chat response. When streaming mode is used (i.e., `stream` option is set to `true`), the response is in MIME type `text/event-stream`. Otherwise, the content type is `application/json`.
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/ChatCompletionResponse'
          examples:
            Example (No Streaming):
              summary: No streaming example
              value:
                choices:
                  - index: 0
                    message:
                      role: assistant
                      content: 'Hello there, how may I assist you today?'
                    finish_reason: stop
                    logprobs: null
                usage:
                  prompt_tokens: 9
                  completion_tokens: 11
                  total_tokens: 20

        text/event-stream:
          x-speakeasy-sse-sentinel: '[DONE]'
          schema:
            $ref: '#/components/schemas/StreamedChatCompletionResponse'
          examples:
            Example (Streaming):
              summary: Streaming example
              value: |
                data: {"choices":[{"index":0,"delta":{"role":"assistant","content":"This"},"finish_reason":null,"logprobs":null}],"usage":null,"created":1726294381}

                data: {"choices":[{"index":0,"delta":{"content":" is"},"finish_reason":null,"logprobs":null}],"usage":null,"created":1726294381}

                ...

                data: {"choices":[{"index":0,"delta":{},"finish_reason":"stop","logprobs":null}],"usage":null,"created":1726294383}

                data: {"choices":[],"usage":{"prompt_tokens":8,"completion_tokens":4,"total_tokens":12},"created":1726294383519714932}

                data: [DONE]

    ToolAssistedChatCompletionSuccess:
      description: Successfully generated a chat response. When streaming mode is used (i.e., `stream` option is set to `true`), the response is in MIME type `text/event-stream`. Otherwise, the content type is `application/json`.
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/ChatCompletionResponse'
          examples:
            Example (No Streaming):
              summary: No streaming example
              value:
                choices:
                  - index: 0
                    message:
                      role: assistant
                      content: 'Hello there, how may I assist you today?'
                    finish_reason: stop
                    logprobs: null
                usage:
                  prompt_tokens: 9
                  completion_tokens: 11
                  total_tokens: 20

        text/event-stream:
          x-speakeasy-sse-sentinel: '[DONE]'
          schema:
            $ref: '#/components/schemas/StreamedToolAssistedChatCompletionResponse'
          examples:
            Example (Streaming):
              summary: Streaming example
              value: |
                event: tool_status
                data: {"tool_call_id":"call_3QrfStXSU6fGdOGPcETocIAh","name":"math:calculator","status":"STARTED","parameters":[{"name":"expression","value":"150 * 1.60934"}],"result":"None","files":null,"message":null,"error":null,"usage":null,"timestamp":1726277121}

                event: tool_status
                data: {"tool_call_id":"call_3QrfStXSU6fGdOGPcETocIAh","name":"math:calculator","status":"ENDED","parameters":[{"name":"expression","value":"150 * 1.60934"}],"result":"\"{\\\"result\\\": \\\"150 * 1.60934=241.401000000000\\\"}\"","files":null,"message":null,"error":null,"usage":null,"timestamp":1726277121}

                data: {"choices":[{"index":0,"delta":{"role":"assistant","content":"To"},"finish_reason":null,"logprobs":null}],"created":1726277121}

                ...

                data: {"choices":[{"index":0,"delta":{"role":"assistant","content":"."},"finish_reason":null,"logprobs":null}],"created":1726277121}

                data: {"choices":[{"index":0,"delta":{},"finish_reason":"stop","logprobs":null}],"created":1726277121}

                data: [DONE]

    CompletionSuccess:
      description: Successfully generated completions. When streaming mode is used (i.e., `stream` option is set to `true`), the response is in MIME type `text/event-stream`. Otherwise, the content type is `application/json`.
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/CompletionResponse'
          examples:
            Example (No Streaming):
              summary: No streaming example
              value:
                choices:
                  - index: 0
                    seed: 42
                    text: 'This is indeed a test'
                    tokens: [128000, 2028, 374, 13118, 264, 1296]
                usage:
                  prompt_tokens: 7
                  completion_tokens: 6
                  total_tokens: 13

        text/event-stream:
          x-speakeasy-sse-sentinel: '[DONE]'
          schema:
            $ref: '#/components/schemas/StreamedCompletionResponse'
          examples:
            Example (Streaming):
              summary: Streaming example (server-sent events)
              value: |
                data: {"event":"token_sampled","index":0,"text":"This","token":2028}

                data: {"event":"token_sampled","index":0,"text":" is","token":374}

                data: {"event":"token_sampled","index":0,"text":" a","token":264}

                data: {"event":"token_sampled","index":0,"text":" test","token":1296}

                data: {"event":"complete","choices":[{"index":0,"seed":6410528593216713765,"text":"This is a test","tokens":[2028,374,264,1296]}],"usage":{"prompt_tokens":8,"completion_tokens":4,"total_tokens":12}}

    TokenizationSuccess:
      description: Successfully tokenized the text.
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/TokenizationResponse'
          examples:
            Example:
              summary: Tokenized output.
              value:
                tokens: [128000, 3923, 374, 1803, 1413, 15592, 30]

    DetokenizationSuccess:
      description: Successfully detokenized the tokens.
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/DetokenizationResponse'
          examples:
            Example:
              summary: Detokenized output.
              value:
                text: 'What is generative AI?'

  securitySchemes:
    bearerAuth:
      type: http
      scheme: bearer
      bearerFormat: JWT
      description: |
        When using Friendli Endpoints API for inference requests, you need to provide a **Friendli Token** for authentication and authorization purposes.

        For more detailed information, please refer [here](https://docs.friendli.ai/openapi/introduction#authentication).

security:
  - bearerAuth: []

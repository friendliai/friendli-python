lockVersion: 2.0.0
id: a1c5fd64-140c-4c5e-a0d3-b834f17979db
management:
  docChecksum: 2c70ec12761d35ff903605a8858a9bb2
  docVersion: v1
  speakeasyVersion: 1.434.7
  generationVersion: 2.452.0
  releaseVersion: 0.6.0
  configChecksum: 5b594f923a7a03f579a9965704666db8
  repoURL: https://github.com/friendliai/friendli-python-internal.git
  installationURL: https://github.com/friendliai/friendli-python-internal.git
features:
  python:
    acceptHeaders: 3.0.0
    additionalDependencies: 1.0.0
    constsAndDefaults: 1.0.4
    core: 5.6.4
    defaultEnabledRetries: 0.2.0
    enumUnions: 0.1.0
    envVarSecurityUsage: 0.3.2
    examples: 3.0.0
    flatRequests: 1.0.1
    flattening: 3.1.0
    globalSecurity: 3.0.2
    globalSecurityCallbacks: 1.0.0
    globalSecurityFlattening: 1.0.0
    globalServerURLs: 3.0.0
    methodArguments: 1.0.2
    methodServerURLs: 3.0.0
    nullables: 1.0.0
    responseFormat: 1.0.1
    retries: 3.0.2
    sdkHooks: 1.0.0
    serverEvents: 1.0.4
    serverEventsSentinels: 0.1.0
    serverIDs: 3.0.0
    tests: 1.6.0
    unions: 3.0.3
generatedFiles:
  - .gitattributes
  - .python-version
  - .vscode/settings.json
  - CONTRIBUTING.md
  - USAGE.md
  - docs/models/assistantmessage.md
  - docs/models/assistantmessagefunction.md
  - docs/models/assistantmessagerole.md
  - docs/models/assistantmessagetype.md
  - docs/models/chatcompletionbody.md
  - docs/models/chatcompletionchoice.md
  - docs/models/chatcompletionchoicefunction.md
  - docs/models/chatcompletionchoicemessage.md
  - docs/models/chatcompletionchoicetoolcalls.md
  - docs/models/chatcompletionchoicetype.md
  - docs/models/chatcompletionrequest.md
  - docs/models/chatcompletionresponse.md
  - docs/models/chatcompletionresult.md
  - docs/models/completionbody.md
  - docs/models/completionbodywithprompt.md
  - docs/models/completionbodywithtokens.md
  - docs/models/completionchoice.md
  - docs/models/completionrequest.md
  - docs/models/completionresponse.md
  - docs/models/completionresult.md
  - docs/models/content.md
  - docs/models/data.md
  - docs/models/delta.md
  - docs/models/detokenizationbody.md
  - docs/models/detokenizationrequest.md
  - docs/models/detokenizationresult.md
  - docs/models/event.md
  - docs/models/filebuiltintool.md
  - docs/models/filebuiltintooltype.md
  - docs/models/function.md
  - docs/models/functiontool.md
  - docs/models/functiontooltype.md
  - docs/models/logitbias.md
  - docs/models/logprobs.md
  - docs/models/message.md
  - docs/models/object.md
  - docs/models/otherbuiltintool.md
  - docs/models/otherbuiltintooltype.md
  - docs/models/parameters.md
  - docs/models/role.md
  - docs/models/security.md
  - docs/models/streamedchatcompletionchoice.md
  - docs/models/streamedchatcompletionchoicefunction.md
  - docs/models/streamedchatcompletionchoicetoolcalls.md
  - docs/models/streamedchatcompletionchoicetype.md
  - docs/models/streamedchatcompletionresult.md
  - docs/models/streamedcompletionresult.md
  - docs/models/streamedcompletionresultdata.md
  - docs/models/streamedcompletiontokencomplete.md
  - docs/models/streamedcompletiontokencompleteevent.md
  - docs/models/streamedcompletiontokensampled.md
  - docs/models/streamedtoolassistedchatcompletionresult.md
  - docs/models/streamedtoolassistedchatcompletionresultdata.md
  - docs/models/streamoptions.md
  - docs/models/systemmessage.md
  - docs/models/textresponseformat.md
  - docs/models/tokenizationbody.md
  - docs/models/tokenizationrequest.md
  - docs/models/tokenizationresult.md
  - docs/models/tokensequence.md
  - docs/models/tool.md
  - docs/models/toolassistedchatcompletionrequest.md
  - docs/models/toolassistedchatcompletionresponse.md
  - docs/models/toolassistedcompletionbody.md
  - docs/models/toolassistedcompletionbodytoolchoice.md
  - docs/models/toolassistedcompletionbodytoolchoicefunction.md
  - docs/models/toolassistedcompletionbodytoolchoicetype.md
  - docs/models/toolcalls.md
  - docs/models/toolchoice.md
  - docs/models/toolchoicefunction.md
  - docs/models/toolchoiceobject.md
  - docs/models/toolchoicetype.md
  - docs/models/toolfortoolassistedchat.md
  - docs/models/toolmessage.md
  - docs/models/toolmessagerole.md
  - docs/models/tooltype.md
  - docs/models/toplogprobs.md
  - docs/models/type.md
  - docs/models/usage.md
  - docs/models/usermessage.md
  - docs/models/usermessagerole.md
  - docs/models/utils/retryconfig.md
  - docs/sdks/friendli/README.md
  - docs/sdks/inference/README.md
  - docs/sdks/serverless/README.md
  - poetry.toml
  - py.typed
  - pylintrc
  - pyproject.toml
  - scripts/prepare-readme.py
  - scripts/publish.sh
  - src/friendli/__init__.py
  - src/friendli/_hooks/__init__.py
  - src/friendli/_hooks/sdkhooks.py
  - src/friendli/_hooks/types.py
  - src/friendli/_version.py
  - src/friendli/basesdk.py
  - src/friendli/httpclient.py
  - src/friendli/inference.py
  - src/friendli/models/__init__.py
  - src/friendli/models/assistantmessage.py
  - src/friendli/models/chatcompletionbody.py
  - src/friendli/models/chatcompletionchoice.py
  - src/friendli/models/chatcompletionop.py
  - src/friendli/models/chatcompletionresult.py
  - src/friendli/models/completionbody.py
  - src/friendli/models/completionbodywithprompt.py
  - src/friendli/models/completionbodywithtokens.py
  - src/friendli/models/completionchoice.py
  - src/friendli/models/completionop.py
  - src/friendli/models/completionresult.py
  - src/friendli/models/detokenizationbody.py
  - src/friendli/models/detokenizationop.py
  - src/friendli/models/detokenizationresult.py
  - src/friendli/models/filebuiltintool.py
  - src/friendli/models/function.py
  - src/friendli/models/functiontool.py
  - src/friendli/models/logprobs.py
  - src/friendli/models/message.py
  - src/friendli/models/otherbuiltintool.py
  - src/friendli/models/sdkerror.py
  - src/friendli/models/security.py
  - src/friendli/models/streamedchatcompletionchoice.py
  - src/friendli/models/streamedchatcompletionresult.py
  - src/friendli/models/streamedcompletionresult.py
  - src/friendli/models/streamedcompletiontokencomplete.py
  - src/friendli/models/streamedcompletiontokensampled.py
  - src/friendli/models/streamedtoolassistedchatcompletionresult.py
  - src/friendli/models/systemmessage.py
  - src/friendli/models/textresponseformat.py
  - src/friendli/models/tokenizationbody.py
  - src/friendli/models/tokenizationop.py
  - src/friendli/models/tokenizationresult.py
  - src/friendli/models/tokensequence.py
  - src/friendli/models/tool.py
  - src/friendli/models/toolassistedchatcompletionop.py
  - src/friendli/models/toolassistedcompletionbody.py
  - src/friendli/models/toolfortoolassistedchat.py
  - src/friendli/models/toolmessage.py
  - src/friendli/models/usage.py
  - src/friendli/models/usermessage.py
  - src/friendli/py.typed
  - src/friendli/sdk.py
  - src/friendli/sdkconfiguration.py
  - src/friendli/serverless.py
  - src/friendli/types/__init__.py
  - src/friendli/types/basemodel.py
  - src/friendli/utils/__init__.py
  - src/friendli/utils/annotations.py
  - src/friendli/utils/enums.py
  - src/friendli/utils/eventstreaming.py
  - src/friendli/utils/forms.py
  - src/friendli/utils/headers.py
  - src/friendli/utils/logger.py
  - src/friendli/utils/metadata.py
  - src/friendli/utils/queryparams.py
  - src/friendli/utils/requestbodies.py
  - src/friendli/utils/retries.py
  - src/friendli/utils/security.py
  - src/friendli/utils/serializers.py
  - src/friendli/utils/url.py
  - src/friendli/utils/values.py
examples:
  ChatCompletion:
    "":
      requestBody:
        application/json: {"model": "meta-llama-3.1-8b-instruct", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Hello!"}], "max_tokens": 200}
      responses:
        "200":
          application/json: {"choices": [], "usage": {"prompt_tokens": 5, "completion_tokens": 7, "total_tokens": 12}}
    Example (No Streaming):
      requestBody:
        application/json: {"model": "meta-llama-3.1-8b-instruct", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Hello!"}], "max_tokens": 200}
      responses:
        "200":
          application/json: {"choices": [{"index": 0, "message": {"role": "assistant", "content": "Hello there, how may I assist you today?"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 9, "completion_tokens": 11, "total_tokens": 20}}
    Example (Streaming):
      requestBody:
        application/json: {"model": "meta-llama-3.1-8b-instruct", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Hello!"}], "max_tokens": 200}
      responses:
        "200":
          text/event-stream: "data: {\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"This\"},\"finish_reason\":null,\"logprobs\":null}],\"usage\":null,\"created\":1726294381}\n\ndata: {\"choices\":[{\"index\":0,\"delta\":{\"content\":\" is\"},\"finish_reason\":null,\"logprobs\":null}],\"usage\":null,\"created\":1726294381}\n\n...\n\ndata: {\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"stop\",\"logprobs\":null}],\"usage\":null,\"created\":1726294383}\n\ndata: {\"choices\":[],\"usage\":{\"prompt_tokens\":8,\"completion_tokens\":4,\"total_tokens\":12},\"created\":1726294383519714932}\n\ndata: [DONE]\n"
  Completion:
    "":
      requestBody:
        application/json: {"prompt": "Say this is a test!", "model": "meta-llama-3.1-8b-instruct", "max_tokens": 200, "top_k": 1}
      responses:
        "200":
          application/json: {"choices": [], "usage": {"prompt_tokens": 5, "completion_tokens": 7, "total_tokens": 12}}
    Example (No Streaming):
      requestBody:
        application/json: {"tokens": [], "model": "meta-llama-3.1-8b-instruct", "max_tokens": 200, "top_k": 1}
      responses:
        "200":
          application/json: {"choices": [{"index": 0, "seed": 42, "text": "This is indeed a test", "tokens": [128000, 2028, 374, 13118, 264, 1296]}], "usage": {"prompt_tokens": 7, "completion_tokens": 6, "total_tokens": 13}}
    Example (Streaming):
      requestBody:
        application/json: {"tokens": [942948, 653298, 515681], "model": "meta-llama-3.1-8b-instruct", "max_tokens": 200, "top_k": 1}
      responses:
        "200":
          text/event-stream: "data: {\"event\":\"token_sampled\",\"index\":0,\"text\":\"This\",\"token\":2028}\n\ndata: {\"event\":\"token_sampled\",\"index\":0,\"text\":\" is\",\"token\":374}\n\ndata: {\"event\":\"token_sampled\",\"index\":0,\"text\":\" a\",\"token\":264}\n\ndata: {\"event\":\"token_sampled\",\"index\":0,\"text\":\" test\",\"token\":1296}\n\ndata: {\"event\":\"complete\",\"choices\":[{\"index\":0,\"seed\":6410528593216713765,\"text\":\"This is a test\",\"tokens\":[2028,374,264,1296]}],\"usage\":{\"prompt_tokens\":8,\"completion_tokens\":4,\"total_tokens\":12}}\n"
  Tokenization:
    "":
      requestBody:
        application/json: {"model": "meta-llama-3.1-8b-instruct", "prompt": "What is generative AI?"}
    Example:
      requestBody:
        application/json: {"model": "meta-llama-3.1-8b-instruct", "prompt": "What is generative AI?"}
      responses:
        "200":
          application/json: {"tokens": [128000, 3923, 374, 1803, 1413, 15592, 30]}
  Detokenization:
    "":
      requestBody:
        application/json: {"model": "meta-llama-3.1-8b-instruct", "tokens": [128000, 3923, 374, 1803, 1413, 15592, 30]}
    Example:
      requestBody:
        application/json: {"model": "meta-llama-3.1-8b-instruct", "tokens": [128000, 3923, 374, 1803, 1413, 15592, 30]}
      responses:
        "200":
          application/json: {"text": "What is generative AI?"}
  ToolAssistedChatCompletion:
    "":
      requestBody:
        application/json: {"model": "meta-llama-3.1-8b-instruct", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Hello!"}], "max_tokens": 200, "tools": [{"type": "math:calculator"}, {"type": "web:url"}]}
      responses:
        "200":
          application/json: {"choices": [{"index": 0, "message": {"role": "<value>"}, "finish_reason": "<value>"}, {"index": 0, "message": {"role": "<value>"}, "finish_reason": "<value>"}], "usage": {"prompt_tokens": 5, "completion_tokens": 7, "total_tokens": 12}}
    Example (No Streaming):
      requestBody:
        application/json: {"model": "meta-llama-3.1-8b-instruct", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Hello!"}], "max_tokens": 200}
      responses:
        "200":
          application/json: {"choices": [{"index": 0, "message": {"role": "assistant", "content": "Hello there, how may I assist you today?"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 9, "completion_tokens": 11, "total_tokens": 20}}
    Example (Streaming):
      requestBody:
        application/json: {"model": "meta-llama-3.1-8b-instruct", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Hello!"}], "max_tokens": 200}
      responses:
        "200":
          text/event-stream: "event: tool_status\ndata: {\"tool_call_id\":\"call_3QrfStXSU6fGdOGPcETocIAh\",\"name\":\"math:calculator\",\"status\":\"STARTED\",\"parameters\":[{\"name\":\"expression\",\"value\":\"150 * 1.60934\"}],\"result\":\"None\",\"files\":null,\"message\":null,\"error\":null,\"usage\":null,\"timestamp\":1726277121}\n\nevent: tool_status\ndata: {\"tool_call_id\":\"call_3QrfStXSU6fGdOGPcETocIAh\",\"name\":\"math:calculator\",\"status\":\"ENDED\",\"parameters\":[{\"name\":\"expression\",\"value\":\"150 * 1.60934\"}],\"result\":\"\\\"{\\\\\\\"result\\\\\\\": \\\\\\\"150 * 1.60934=241.401000000000\\\\\\\"}\\\"\",\"files\":null,\"message\":null,\"error\":null,\"usage\":null,\"timestamp\":1726277121}\n\ndata: {\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"To\"},\"finish_reason\":null,\"logprobs\":null}],\"created\":1726277121}\n\n...\n\ndata: {\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\".\"},\"finish_reason\":null,\"logprobs\":null}],\"created\":1726277121}\n\ndata: {\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"stop\",\"logprobs\":null}],\"created\":1726277121}\n\ndata: [DONE]\n"
